# nlp-roadmap

🗺️Natural Language Processing Roadmap.

> ⚠️NOTE:
> 
> 1. This project is an experiment called `PCB`, PCB is not Printed Circuit Board, but an abbreviation of `Paper Code Blog`. I think these three things allow us to quickly master a point of knowledge, taking into account both theory and practice.
> 
> 2. The stars at the end of the paper indicate the importance.
>     1. 🌟: genernal.
>     2. 🌟🌟: important.
>     3. 🌟🌟🌟: especially important.

## 1 Word Embedding

### Survey

- paper: [Word Embeddings: A Survey](https://arxiv.org/abs/1901.09069) 🌟🌟🌟

### NNLM

- paper: [A Neural Probabilistic Language Model](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) 🌟
- code: [NNLM](https://github.com/FuYanzhe2/NNLM) ![](https://img.shields.io/github/stars/FuYanzhe2/NNLM.svg)
- blog: [A Neural Probabilistic Language Model](https://zhuanlan.zhihu.com/p/21240807)

### W2V

- paper: [Efficient Estiation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781) 🌟🌟

### Glove

- paper: [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf) 🌟🌟

### FastText

- paper: [Bag of Tricks for Efficient Text Classification](https://arxiv.org/pdf/1607.01759.pdf) 🌟🌟

### ELMO

- paper: [Deep contextualized word representations](https://arxiv.org/pdf/1802.05365.pdf) 🌟🌟

### GPT

- paper: [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) 🌟🌟

### BERT

- paper: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf) 🌟🌟🌟
- code: [bert](https://github.com/google-research/bert) ![](https://img.shields.io/github/stars/google-research/bert.svg)
- blog: [从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史](https://zhuanlan.zhihu.com/p/49271699)

