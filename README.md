# nlp-roadmap

ðŸ—ºï¸Natural Language Processing Roadmap.

> This project is an experiment called PCB, PCB is not **Printed Circuit Board**, but an abbreviation of `Paper Code Blog`. I think these three things allow us to quickly master a point of knowledge, taking into account both theory and practice.


## 1 Word Embedding

### NNLM 

- paper: [A Neural Probabilistic Language Model](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)
- code: [NNLM](https://github.com/FuYanzhe2/NNLM)
- blog: [A Neural Probabilistic Language Model](https://zhuanlan.zhihu.com/p/21240807)

### Word2Vector

- paper: [Efficient Estiation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)
  
### Glove

- paper: [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)

### FastText

- paper: [Bag of Tricks for Efficient Text Classification](https://arxiv.org/pdf/1607.01759.pdf)

### ELMO

- paper: [Deep contextualized word representations](https://arxiv.org/pdf/1802.05365.pdf)

### GPT

- paper: [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)

### BERT

5. [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)
