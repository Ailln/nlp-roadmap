# nlp-roadmap

ğŸ—ºï¸Natural Language Processing Roadmap.

> This project is an experiment called `PCB`, PCB is not Printed Circuit Board, but an abbreviation of `Paper Code Blog`. I think these three things allow us to quickly master a point of knowledge, taking into account both theory and practice.


## 1 Word Embedding

### Survey

- paper: [Word Embeddings: A Survey](https://arxiv.org/abs/1901.09069)

### NNLM

- paper: [A Neural Probabilistic Language Model](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)
- code: [NNLM](https://github.com/FuYanzhe2/NNLM) ![](https://img.shields.io/github/stars/FuYanzhe2/NNLM.svg)
- blog: [A Neural Probabilistic Language Model](https://zhuanlan.zhihu.com/p/21240807)

### W2V

- paper: [Efficient Estiation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)

### Glove

- paper: [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)

### FastText

- paper: [Bag of Tricks for Efficient Text Classification](https://arxiv.org/pdf/1607.01759.pdf)

### ELMO

- paper: [Deep contextualized word representations](https://arxiv.org/pdf/1802.05365.pdf)

### GPT

- paper: [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)

### BERT

- paper: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)
- code: [bert](https://github.com/google-research/bert) ![](https://img.shields.io/github/stars/google-research/bert.svg)
- blog: [ä»Word Embeddingåˆ°Bertæ¨¡å‹â€”è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„é¢„è®­ç»ƒæŠ€æœ¯å‘å±•å²](https://zhuanlan.zhihu.com/p/49271699)

