# Natural Language Processing Roadmap

ğŸ—ºï¸ ä¸€ä¸ªè‡ªç„¶è¯­è¨€å¤„ç†çš„å­¦ä¹ è·¯çº¿å›¾

> âš ï¸ æ³¨æ„:
>
> 1. è¿™ä¸ªé¡¹ç›®åŒ…å«ä¸€ä¸ªåä¸º `PCB` çš„å°å®éªŒï¼Œè¿™ä¸ªçš„ PCB ä¸æ˜¯å°åˆ·ç”µè·¯æ¿ `Printed Circuit Board`ï¼Œä¹Ÿä¸æ˜¯è¿›ç¨‹æ§åˆ¶å— `Process Control Block`ï¼Œè€Œæ˜¯ `Paper Code Blog` çš„ç¼©å†™ã€‚æˆ‘è®¤ä¸º `è®ºæ–‡`ã€`ä»£ç ` å’Œ `åšå®¢` è¿™ä¸‰ä¸ªä¸œè¥¿ï¼Œå¯ä»¥è®©æˆ‘ä»¬å…¼é¡¾ç†è®ºå’Œå®è·µåŒæ—¶ï¼Œå¿«é€Ÿåœ°æŒæ¡çŸ¥è¯†ç‚¹ï¼
>
> 2. æ¯ç¯‡è®ºæ–‡åé¢çš„æ˜Ÿæ˜Ÿä¸ªæ•°ä»£è¡¨è®ºæ–‡çš„é‡è¦æ€§ï¼ˆ*ä¸»è§‚æ„è§ï¼Œä»…ä¾›å‚è€ƒ*ï¼‰ã€‚
>     1. ğŸŒŸ: ä¸€èˆ¬ï¼›
>     2. ğŸŒŸğŸŒŸ: é‡è¦ï¼›
>     3. ğŸŒŸğŸŒŸğŸŒŸ: éå¸¸é‡è¦ã€‚

## 1 åˆ†è¯ `Word Segmentation`

**è¯æ˜¯èƒ½å¤Ÿç‹¬ç«‹æ´»åŠ¨çš„æœ€å°è¯­è¨€å•ä½ã€‚** åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ï¼Œé€šå¸¸éƒ½æ˜¯ä»¥è¯ä½œä¸ºåŸºæœ¬å•ä½è¿›è¡Œå¤„ç†çš„ã€‚ç”±äºè‹±æ–‡æœ¬èº«å…·æœ‰å¤©ç”Ÿçš„ä¼˜åŠ¿ï¼Œä»¥ç©ºæ ¼åˆ’åˆ†æ‰€æœ‰è¯ã€‚è€Œä¸­æ–‡çš„è¯ä¸è¯ä¹‹é—´æ²¡æœ‰æ˜æ˜¾çš„åˆ†å‰²æ ‡è®°ï¼Œæ‰€ä»¥åœ¨åšä¸­æ–‡è¯­è¨€å¤„ç†å‰çš„é¦–è¦ä»»åŠ¡ï¼Œå°±æ˜¯æŠŠè¿ç»­ä¸­æ–‡å¥å­åˆ†å‰²æˆã€Œè¯åºåˆ—ã€ã€‚è¿™ä¸ªåˆ†å‰²çš„è¿‡ç¨‹å°±å«**åˆ†è¯**ã€‚[äº†è§£æ›´å¤š](https://www.v2ai.cn/2018/04/26/nature-language-processing/2-word-segmentation/)

### ç»¼è¿°

- æ±‰è¯­åˆ†è¯æŠ€æœ¯ç»¼è¿° [{Paper}](http://www.lis.ac.cn/CN/article/downloadArticleFile.do?attachType=PDF&id=9402) ğŸŒŸ
- å›½å†…ä¸­æ–‡è‡ªåŠ¨åˆ†è¯æŠ€æœ¯ç ”ç©¶ç»¼è¿° [{Paper}](http://www.lis.ac.cn/CN/article/downloadArticleFile.do?attachType=PDF&id=11361) ğŸŒŸ
- æ±‰è¯­è‡ªåŠ¨åˆ†è¯çš„ç ”ç©¶ç°çŠ¶ä¸å›°éš¾ [{Paper}](http://sourcedb.ict.cas.cn/cn/ictthesis/200907/P020090722605434114544.pdf) ğŸŒŸğŸŒŸ
- æ±‰è¯­è‡ªåŠ¨åˆ†è¯ç ”ç©¶è¯„è¿° [{Paper}](http://59.108.48.5/course/mining/12-13spring/%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE/02-01%E6%B1%89%E8%AF%AD%E8%87%AA%E5%8A%A8%E5%88%86%E8%AF%8D%E7%A0%94%E7%A9%B6%E8%AF%84%E8%BF%B0.pdf) ğŸŒŸğŸŒŸ
- ä¸­æ–‡åˆ†è¯åå¹´åˆå›é¡¾: 2007-2017 [{Paper}](https://arxiv.org/pdf/1901.06079.pdf) ğŸŒŸğŸŒŸğŸŒŸ
- chinese-word-segmentation [{Code}](https://github.com/Ailln/chinese-word-segmentation)
- æ·±åº¦å­¦ä¹ ä¸­æ–‡åˆ†è¯è°ƒç ” [{Blog}](http://www.hankcs.com/nlp/segment/depth-learning-chinese-word-segmentation-survey.html)

## 2 è¯åµŒå…¥ `Word Embedding`

**è¯åµŒå…¥**å°±æ˜¯æ‰¾åˆ°ä¸€ä¸ªæ˜ å°„æˆ–è€…å‡½æ•°ï¼Œç”Ÿæˆåœ¨ä¸€ä¸ªæ–°çš„ç©ºé—´ä¸Šçš„è¡¨ç¤ºï¼Œè¯¥è¡¨ç¤ºè¢«ç§°ä¸ºã€Œå•è¯è¡¨ç¤ºã€ã€‚[äº†è§£æ›´å¤š](https://www.v2ai.cn/2018/08/27/nature-language-processing/6-word-embedding/)

### ç»¼è¿°

- Word Embeddings: A Survey [{Paper}](https://arxiv.org/pdf/1901.09069.pdf) ğŸŒŸğŸŒŸğŸŒŸ
- Visualizing Attention in Transformer-Based Language Representation Models [{Paper}](https://arxiv.org/pdf/1904.02679.pdf) ğŸŒŸğŸŒŸ
- **PTMs**: Pre-trained Models for Natural Language Processing: A Survey [{Paper}](https://arxiv.org/pdf/2003.08271.pdf) [{Blog}](https://zhuanlan.zhihu.com/p/115014536) ğŸŒŸğŸŒŸğŸŒŸ
- Efficient Transformers: A Survey [{Paper}](https://arxiv.org/pdf/2009.06732.pdf) ğŸŒŸğŸŒŸ
- A Survey of Transformers [{Paper}](https://arxiv.org/pdf/2106.04554.pdf) ğŸŒŸğŸŒŸ
- Pre-Trained Models: Past, Present and Future [{Paper}](https://arxiv.org/pdf/2106.07139.pdf) ğŸŒŸğŸŒŸ
- Pretrained Language Models for Text Generation: A Survey [{Paper}](https://arxiv.org/pdf/2105.10311.pdf) ğŸŒŸ
- A Practical Survey on Faster and Lighter Transformers [{Paper}](https://arxiv.org/pdf/2103.14636.pdf) ğŸŒŸ
- The NLP Cookbook: Modern Recipes for Transformer based Deep Learning Architectures [{Paper}](https://arxiv.org/pdf/2104.10640.pdf) ğŸŒŸğŸŒŸ

### æ ¸å¿ƒ

- **NNLM**: A Neural Probabilistic Language Model [{Paper}](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) [{Code}](https://github.com/FuYanzhe2/NNLM) [{Blog}](https://zhuanlan.zhihu.com/p/21240807) ğŸŒŸ
- **W2V**: Efficient Estiation of Word Representations in Vector Space [{Paper}](https://arxiv.org/abs/1301.3781) ğŸŒŸğŸŒŸ
- **Glove**: Global Vectors for Word Representation [{Paper}](https://nlp.stanford.edu/pubs/glove.pdf) ğŸŒŸğŸŒŸ
- **CharCNN**: Character-level Convolutional Networks for Text Classification [{Paper}](https://arxiv.org/pdf/1509.01626.pdf) [{Blog}](https://zhuanlan.zhihu.com/p/51698513) ğŸŒŸ
- **ULMFiT**: Universal Language Model Fine-tuning for Text Classification [{Paper}](https://arxiv.org/pdf/1801.06146.pdf) ğŸŒŸ
- **SiATL**: An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models [{Paper}](https://www.aclweb.org/anthology/N19-1213.pdf) ğŸŒŸ
- **FastText**: Bag of Tricks for Efficient Text Classification [{Paper}](https://arxiv.org/pdf/1607.01759.pdf) ğŸŒŸğŸŒŸ
- **CoVe**: Learned in Translation: Contextualized Word Vectors [{Paper}](https://arxiv.org/pdf/1708.00107.pdf) ğŸŒŸ
- **ELMo**: Deep contextualized word representations [{Paper}](https://arxiv.org/pdf/1802.05365.pdf) ğŸŒŸğŸŒŸ
- **Transformer**: Attention is All you Need [{Paper}](https://arxiv.org/pdf/1706.03762.pdf) [{Code}](https://github.com/tensorflow/tensor2tensor) [{Blog}](http://jalammar.github.io/illustrated-transformer/) ğŸŒŸğŸŒŸğŸŒŸ
- **GPT**: Improving Language Understanding by Generative Pre-Training [{Paper}](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) ğŸŒŸ
- **GPT2**: Language Models are Unsupervised Multitask Learners [{Paper}](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf) [{Code}](https://github.com/openai/gpt-2) [{Blog}](https://openai.com/blog/better-language-models/) ğŸŒŸğŸŒŸ
- **GPT3**: Language Models are Few-Shot Learners [{Paper}](https://arxiv.org/pdf/2005.14165.pdf) [{Code}](https://github.com/openai/gpt-3) ğŸŒŸğŸŒŸğŸŒŸ
- **BERT**: Pre-training of Deep Bidirectional Transformers for Language Understanding [{Paper}](https://arxiv.org/pdf/1810.04805.pdf) [{Code}](https://github.com/google-research/bert) [{Blog}](https://zhuanlan.zhihu.com/p/49271699) ğŸŒŸğŸŒŸğŸŒŸ
- **UniLM**: Unified Language Model Pre-training for Natural Language Understanding and Generation [{Paper}](https://arxiv.org/pdf/1905.03197.pdf) [{Code}](https://github.com/microsoft/unilm) [{Blog}](https://zhuanlan.zhihu.com/p/68327602) ğŸŒŸğŸŒŸ
- **T5**: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer [{Paper}](https://arxiv.org/pdf/1910.10683.pdf) [{Code}](https://github.com/google-research/text-to-text-transfer-transformer) [{Blog}](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) ğŸŒŸ
- **ERNIE**(Baidu): Enhanced Representation through Knowledge Integration [{Paper}](https://arxiv.org/pdf/1904.09223.pdf) [{Code}](https://github.com/PaddlePaddle/ERNIE) ğŸŒŸ
- **ERNIE**(Tsinghua): Enhanced Language Representation with Informative Entities [{Paper}](https://arxiv.org/pdf/1905.07129.pdf) [{Code}](https://github.com/thunlp/ERNIE) ğŸŒŸ
- **RoBERTa**: A Robustly Optimized BERT Pretraining Approach [{Paper}](https://arxiv.org/pdf/1907.11692.pdf) ğŸŒŸ
- **ALBERT**: A Lite BERT for Self-supervised Learning of Language Representations [{Paper}](https://arxiv.org/pdf/1909.11942.pdf) [{Code}](https://github.com/google-research/ALBERT) ğŸŒŸğŸŒŸ
- **TinyBERT**: Distilling BERT for Natural Language Understanding [{Paper}](https://arxiv.org/pdf/1909.10351.pdf) ğŸŒŸğŸŒŸ
- **FastFormers**: Highly Efficient Transformer Models for Natural Language Understanding [{Paper}](https://arxiv.org/pdf/2010.13382.pdf) [{Code}](https://github.com/microsoft/fastformers) ğŸŒŸğŸŒŸ

### å…¶ä»–

- word2vec Parameter Learning Explained [{Paper}](https://arxiv.org/pdf/1411.2738.pdf) ğŸŒŸğŸŒŸ
- Semi-supervised Sequence Learning [{Paper}](https://arxiv.org/pdf/1511.01432.pdf) ğŸŒŸğŸŒŸ
- BERT Rediscovers the Classical NLP Pipeline [{Paper}](https://arxiv.org/pdf/1905.05950.pdf) ğŸŒŸ
- Pre-trained Languge Model Papers [{Blog}](https://github.com/thunlp/PLMpapers)
- HuggingFace Transformers [{Code}](https://github.com/huggingface/transformers)
- Fudan FastNLP [{Code}](https://github.com/fastnlp/fastNLP)

## 3 åºåˆ—æ ‡æ³¨ `Sequence Labeling`

### ç»¼è¿°

- Sequence Labeling çš„å‘å±•å²ï¼ˆDNNs+CRFï¼‰[{Blog}](https://zhuanlan.zhihu.com/p/34828874)

### Bi-LSTM + CRF

- End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF [{Paper}](https://www.aclweb.org/anthology/P16-1101) ğŸŒŸğŸŒŸ

- pytorch_NER_BiLSTM_CNN_CRF [{Code}](https://github.com/bamtercelboo/pytorch_NER_BiLSTM_CNN_CRF)
- NN_NER_tensorFlow [{Code}](https://github.com/LopezGG/NN_NER_tensorFlow)
- End-to-end-Sequence-Labeling-via-Bi-directional-LSTM-CNNs-CRF-Tutorial [{Code}](https://github.com/jayavardhanr/End-to-end-Sequence-Labeling-via-Bi-directional-LSTM-CNNs-CRF-Tutorial)
- Bi-directional LSTM-CNNs-CRF [{Code}](https://zhuanlan.zhihu.com/p/30791481)

### å…¶ä»–

- Sequence to Sequence Learning with Neural Networks [{Paper}](https://proceedings.neurips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf) ğŸŒŸ
- Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks [{Paper}](https://arxiv.org/pdf/1506.03099.pdf) ğŸŒŸ

## 4 çŸ¥è¯†å›¾è°± `Knowledge Graph`

### ç»¼è¿°

- Towards a Definition of Knowledge Graphs [{Paper}](http://ceur-ws.org/Vol-1695/paper4.pdf) ğŸŒŸğŸŒŸğŸŒŸ

## 5 å¯¹è¯ç³»ç»Ÿ `Dialogue Systems`

### ç»¼è¿°

- A Survey on Dialogue Systems: Recent Advances and New Frontiers [{Paper}](https://arxiv.org/pdf/1711.01731v1.pdf) [{Blog}](https://zhuanlan.zhihu.com/p/45210996) ğŸŒŸğŸŒŸ
- å°å“¥å“¥ï¼Œæ£€ç´¢å¼chatbotäº†è§£ä¸€ä¸‹ï¼Ÿ [{Blog}](https://mp.weixin.qq.com/s/yC8uYwti9Meyt83xkmbmcg) ğŸŒŸğŸŒŸğŸŒŸ

### Task-Oriented Dialogue Systems

- **Joint NLU**: Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling [{Paper}](https://arxiv.org/pdf/1609.01454.pdf) [{Code}](https://github.com/Ailln/chatbot) ğŸŒŸğŸŒŸ
- BERT for Joint Intent Classification and Slot Filling [{Paper}](https://arxiv.org/pdf/1902.10909.pdf) ğŸŒŸ
- Sequicity: Simplifying Task-oriented Dialogue Systems with Single Sequence-to-Sequence Architectures [{Paper}](https://www.aclweb.org/anthology/P18-1133.pdf) [{Code}](https://github.com/WING-NUS/sequicity) ğŸŒŸğŸŒŸ

### Conversational Response Selection

- Multi-view Response Selection for Human-Computer Conversation [{Paper}](aclweb.org/anthology/D16-1036.pdf) ğŸŒŸğŸŒŸ
- **SMN**: Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-Based Chatbots [{Paper}](https://www.aclweb.org/anthology/P17-1046.pdf) [{Code}](https://github.com/MarkWuNLP/MultiTurnResponseSelection) [{Blog}](https://zhuanlan.zhihu.com/p/65062025) ğŸŒŸğŸŒŸğŸŒŸ
- **DUA**: Modeling Multi-turn Conversation with Deep Utterance Aggregation [{Paper}](https://www.aclweb.org/anthology/C18-1317.pdf) [{Code}](https://github.com/cooelf/DeepUtteranceAggregation) [{Blog}](https://zhuanlan.zhihu.com/p/60618158) ğŸŒŸğŸŒŸ
- **DAM**: Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network [{Paper}](https://www.aclweb.org/anthology/P18-1103.pdf) [{Code}](https://github.com/baidu/Dialogue/tree/master/DAM) [{Blog}](https://zhuanlan.zhihu.com/p/65143297) ğŸŒŸğŸŒŸğŸŒŸ
- **IMN**: Interactive Matching Network for Multi-Turn Response Selection in Retrieval-Based Chatbots [{Paper}](https://arxiv.org/pdf/1901.01824.pdf) [{Code}](https://github.com/JasonForJoy/IMN) [{Blog}](https://zhuanlan.zhihu.com/p/68590678) ğŸŒŸğŸŒŸ

## 6 ä¸»é¢˜æ¨¡å‹ `Topic Model`

### LDA

- Latent Dirichlet Allocation  [{Paper}](https://jmlr.org/papers/volume3/blei03a/blei03a.pdf) [{Blog}](https://arxiv.org/pdf/1908.03142.pdf) ğŸŒŸğŸŒŸğŸŒŸ

## 7 æç¤ºå­¦ä¹  `Prompt Learning`

### ç»¼è¿°

- **PPP**: Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing [{Paper}](https://arxiv.org/pdf/2107.13586.pdf) [{Blog}](https://zhuanlan.zhihu.com/p/395115779) ğŸŒŸğŸŒŸğŸŒŸ

## 8 å›¾ç¥ç»ç½‘ç»œ

### ç»¼è¿°

- Graph Neural Networks for Natural Language Processing: A Survey [{Paper}](https://arxiv.org/pdf/2106.06090.pdf) ğŸŒŸğŸŒŸ

## å‚è€ƒ

- [thunlp/NLP-THU](https://github.com/thunlp/NLP-THU)
- [iwangjian/Paper-Reading](https://github.com/iwangjian/Paper-Reading)
- [thunlp/PromptPapers](https://github.com/thunlp/PromptPapers)
